{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些基本的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nuplan.planning.scenario_builder.scenario_filter import ScenarioFilter\n",
    "from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_utils import ScenarioMapping\n",
    "from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_builder import NuPlanScenarioBuilder\n",
    "from nuplan.planning.utils.multithreading.worker_parallel import SingleMachineParallelExecutor\n",
    "from nuplan.planning.training.preprocessing.utils.vector_preprocessing import interpolate_points\n",
    "from nuplan.common.geometry.torch_geometry import vector_set_coordinates_to_local_frame\n",
    "from nuplan.common.actor_state.tracked_objects_types import TrackedObjectType\n",
    "from nuplan.planning.training.preprocessing.features.trajectory_utils import convert_absolute_to_relative_poses\n",
    "from nuplan.common.geometry.torch_geometry import global_state_se2_tensor_to_local\n",
    "from nuplan.planning.training.preprocessing.features.agents import Agents\n",
    "from nuplan.planning.training.preprocessing.utils.agents_preprocessing import (\n",
    "    AgentInternalIndex,\n",
    "    AgentFeatureIndex,\n",
    "    EgoInternalIndex,\n",
    "    sampled_past_ego_states_to_tensor,\n",
    "    sampled_past_timestamps_to_tensor,\n",
    "    compute_yaw_rate_from_state_tensors,\n",
    "    filter_agents_tensor,\n",
    "    pack_agents_tensor,\n",
    "    pad_agent_states\n",
    ")\n",
    "\n",
    "from nuplan.planning.training.preprocessing.feature_builders.vector_builder_utils import (\n",
    "    MapObjectPolylines,\n",
    "    LaneSegmentTrafficLightData,\n",
    "    VectorFeatureLayer, \n",
    "     VectorFeatureLayerMapping,\n",
    "    get_lane_polylines,\n",
    "    get_traffic_light_encoding,\n",
    "     get_route_lane_polylines_from_roadblock_ids,\n",
    "    get_map_object_polygons\n",
    ")\n",
    "\n",
    "from nuplan.common.maps.maps_datatypes import TrafficLightStatusData\n",
    "from nuplan.common.actor_state.state_representation import Point2D, StateSE2\n",
    "from nuplan.common.maps.abstract_map import AbstractMap\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def get_filter_parameters(num_scenarios_per_type=20, limit_total_scenarios=None, shuffle=True):\n",
    "    # nuplan challenge\n",
    "    scenario_types = [\n",
    "        'starting_left_turn',\n",
    "        'starting_right_turn',\n",
    "        'starting_straight_traffic_light_intersection_traversal',\n",
    "        'stopping_with_lead',\n",
    "        'high_lateral_acceleration',\n",
    "        'high_magnitude_speed',\n",
    "        'low_magnitude_speed',\n",
    "        'traversing_pickup_dropoff',\n",
    "        'waiting_for_pedestrian_to_cross',\n",
    "        'behind_long_vehicle',\n",
    "        'stationary_in_traffic',\n",
    "        'near_multiple_vehicles',\n",
    "        'changing_lane',\n",
    "        'following_lane_with_lead',\n",
    "    ]\n",
    "\n",
    "    scenario_tokens = None              # List of scenario tokens to include\n",
    "    log_names = None                     # Filter scenarios by log names\n",
    "    map_names = None                     # Filter scenarios by map names\n",
    "\n",
    "    num_scenarios_per_type               # Number of scenarios per type\n",
    "    limit_total_scenarios                # Limit total scenarios (float = fraction, int = num) - this filter can be applied on top of num_scenarios_per_type\n",
    "    timestamp_threshold_s = None          # Filter scenarios to ensure scenarios have more than `timestamp_threshold_s` seconds between their initial lidar timestamps\n",
    "    ego_displacement_minimum_m = None    # Whether to remove scenarios where the ego moves less than a certain amount\n",
    "\n",
    "    expand_scenarios = False           # Whether to expand multi-sample scenarios to multiple single-sample scenarios\n",
    "    remove_invalid_goals = True         # Whether to remove scenarios where the mission goal is invalid\n",
    "    shuffle                             # Whether to shuffle the scenarios\n",
    "\n",
    "    ego_start_speed_threshold = None     # Limit to scenarios where the ego reaches a certain speed from below\n",
    "    ego_stop_speed_threshold = None      # Limit to scenarios where the ego reaches a certain speed from above\n",
    "    speed_noise_tolerance = None         # Value at or below which a speed change between two timepoints should be ignored as noise.\n",
    "\n",
    "    return scenario_types, scenario_tokens, log_names, map_names, num_scenarios_per_type, limit_total_scenarios, timestamp_threshold_s, ego_displacement_minimum_m, \\\n",
    "           expand_scenarios, remove_invalid_goals, shuffle, ego_start_speed_threshold, ego_stop_speed_threshold, speed_noise_tolerance\n",
    "\n",
    "def get_scenario_map():\n",
    "    scenario_map = {\n",
    "        'accelerating_at_crosswalk': [15.0, -3.0],\n",
    "        'accelerating_at_stop_sign': [15.0, -3.0],\n",
    "        'accelerating_at_stop_sign_no_crosswalk': [15.0, -3.0],\n",
    "        'accelerating_at_traffic_light': [15.0, -3.0],\n",
    "        'accelerating_at_traffic_light_with_lead': [15.0, -3.0],\n",
    "        'accelerating_at_traffic_light_without_lead': [15.0, -3.0],\n",
    "        'behind_bike': [15.0, -3.0],\n",
    "        'behind_long_vehicle': [15.0, -3.0],\n",
    "        'behind_pedestrian_on_driveable': [15.0, -3.0],\n",
    "        'behind_pedestrian_on_pickup_dropoff': [15.0, -3.0],\n",
    "        'changing_lane': [15.0, -3.0],\n",
    "        'changing_lane_to_left': [15.0, -3.0],\n",
    "        'changing_lane_to_right': [15.0, -3.0],\n",
    "        'changing_lane_with_lead': [15.0, -3.0],\n",
    "        'changing_lane_with_trail': [15.0, -3.0],\n",
    "        'crossed_by_bike': [15.0, -3.0],\n",
    "        'crossed_by_vehicle': [15.0, -3.0],\n",
    "        'following_lane_with_lead': [15.0, -3.0],\n",
    "        'following_lane_with_slow_lead': [15.0, -3.0],\n",
    "        'following_lane_without_lead': [15.0, -3.0],\n",
    "        'high_lateral_acceleration': [15.0, -3.0],\n",
    "        'high_magnitude_jerk': [15.0, -3.0],\n",
    "        'high_magnitude_speed': [15.0, -3.0],\n",
    "        'low_magnitude_speed': [15.0, -3.0],\n",
    "        'medium_magnitude_speed': [15.0, -3.0],\n",
    "        'near_barrier_on_driveable': [15.0, -3.0],\n",
    "        'near_construction_zone_sign': [15.0, -3.0],\n",
    "        'near_high_speed_vehicle': [15.0, -3.0],\n",
    "        'near_long_vehicle': [15.0, -3.0],\n",
    "        'near_multiple_bikes': [15.0, -3.0],\n",
    "        'near_multiple_pedestrians': [15.0, -3.0],\n",
    "        'near_multiple_vehicles': [15.0, -3.0],\n",
    "        'near_pedestrian_at_pickup_dropoff': [15.0, -3.0],\n",
    "        'near_pedestrian_on_crosswalk': [15.0, -3.0],\n",
    "        'near_pedestrian_on_crosswalk_with_ego': [15.0, -3.0],\n",
    "        'near_trafficcone_on_driveable': [15.0, -3.0],\n",
    "        'on_all_way_stop_intersection': [15.0, -3.0],\n",
    "        'on_carpark': [15.0, -3.0],\n",
    "        'on_intersection': [15.0, -3.0],\n",
    "        'on_pickup_dropoff': [15.0, -3.0],\n",
    "        'on_stopline_crosswalk': [15.0, -3.0],\n",
    "        'on_stopline_stop_sign': [15.0, -3.0],\n",
    "        'on_stopline_traffic_light': [15.0, -3.0],\n",
    "        'on_traffic_light_intersection': [15.0, -3.0],\n",
    "        'starting_high_speed_turn': [15.0, -3.0],\n",
    "        'starting_left_turn': [15.0, -3.0],\n",
    "        'starting_low_speed_turn': [15.0, -3.0],\n",
    "        'starting_protected_cross_turn': [15.0, -3.0],\n",
    "        'starting_protected_noncross_turn': [15.0, -3.0],\n",
    "        'starting_right_turn': [15.0, -3.0],\n",
    "        'starting_straight_stop_sign_intersection_traversal': [15.0, -3.0],\n",
    "        'starting_straight_traffic_light_intersection_traversal': [15.0, -3.0],\n",
    "        'starting_u_turn': [15.0, -3.0],\n",
    "        'starting_unprotected_cross_turn': [15.0, -3.0],\n",
    "        'starting_unprotected_noncross_turn': [15.0, -3.0],\n",
    "        'stationary': [15.0, -3.0],\n",
    "        'stationary_at_crosswalk': [15.0, -3.0],\n",
    "        'stationary_at_traffic_light_with_lead': [15.0, -3.0],\n",
    "        'stationary_at_traffic_light_without_lead': [15.0, -3.0],\n",
    "        'stationary_in_traffic': [15.0, -3.0],\n",
    "        'stopping_at_crosswalk': [15.0, -3.0],\n",
    "        'stopping_at_stop_sign_no_crosswalk': [15.0, -3.0],\n",
    "        'stopping_at_stop_sign_with_lead': [15.0, -3.0],\n",
    "        'stopping_at_stop_sign_without_lead': [15.0, -3.0],\n",
    "        'stopping_at_traffic_light_with_lead': [15.0, -3.0],\n",
    "        'stopping_at_traffic_light_without_lead': [15.0, -3.0],\n",
    "        'stopping_with_lead': [15.0, -3.0],\n",
    "        'traversing_crosswalk': [15.0, -3.0],\n",
    "        'traversing_intersection': [15.0, -3.0],\n",
    "        'traversing_narrow_lane': [15.0, -3.0],\n",
    "        'traversing_pickup_dropoff': [15.0, -3.0],\n",
    "        'traversing_traffic_light_intersection': [15.0, -3.0],\n",
    "        'waiting_for_pedestrian_to_cross': [15.0, -3.0]\n",
    "    }\n",
    "\n",
    "    return scenario_map\n",
    "\n",
    "def _extract_agent_tensor(tracked_objects, track_token_ids, object_types):\n",
    "    \"\"\"\n",
    "    Extracts the relevant data from the agents present in a past detection into a tensor.\n",
    "    Only objects of specified type will be transformed. Others will be ignored.\n",
    "    The output is a tensor as described in AgentInternalIndex\n",
    "    :param tracked_objects: The tracked objects to turn into a tensor.\n",
    "    :track_token_ids: A dictionary used to assign track tokens to integer IDs.\n",
    "    :object_type: TrackedObjectType to filter agents by.\n",
    "    :return: The generated tensor and the updated track_token_ids dict.\n",
    "    \"\"\"\n",
    "    agents = tracked_objects.get_tracked_objects_of_types(object_types)\n",
    "    agent_types = []\n",
    "    output = torch.zeros((len(agents), AgentInternalIndex.dim()), dtype=torch.float32)\n",
    "    max_agent_id = len(track_token_ids)\n",
    "\n",
    "    for idx, agent in enumerate(agents):\n",
    "        if agent.track_token not in track_token_ids:\n",
    "            track_token_ids[agent.track_token] = max_agent_id\n",
    "            max_agent_id += 1\n",
    "        track_token_int = track_token_ids[agent.track_token]\n",
    "\n",
    "        output[idx, AgentInternalIndex.track_token()] = float(track_token_int)\n",
    "        output[idx, AgentInternalIndex.vx()] = agent.velocity.x\n",
    "        output[idx, AgentInternalIndex.vy()] = agent.velocity.y\n",
    "        output[idx, AgentInternalIndex.heading()] = agent.center.heading\n",
    "        output[idx, AgentInternalIndex.width()] = agent.box.width\n",
    "        output[idx, AgentInternalIndex.length()] = agent.box.length\n",
    "        output[idx, AgentInternalIndex.x()] = agent.center.x\n",
    "        output[idx, AgentInternalIndex.y()] = agent.center.y\n",
    "        agent_types.append(agent.tracked_object_type)\n",
    "\n",
    "    return output, track_token_ids, agent_types\n",
    "\n",
    "def sampled_tracked_objects_to_tensor_list(past_tracked_objects):\n",
    "    \"\"\"\n",
    "    Tensorizes the agents features from the provided past detections.\n",
    "    For N past detections, output is a list of length N, with each tensor as described in `_extract_agent_tensor()`.\n",
    "    :param past_tracked_objects: The tracked objects to tensorize.\n",
    "    :return: The tensorized objects.\n",
    "    \"\"\"\n",
    "    object_types = [TrackedObjectType.VEHICLE, TrackedObjectType.PEDESTRIAN, TrackedObjectType.BICYCLE]\n",
    "    output = []\n",
    "    output_types = []\n",
    "    track_token_ids = {}\n",
    "\n",
    "    for i in range(len(past_tracked_objects)):\n",
    "        tensorized, track_token_ids, agent_types = _extract_agent_tensor(past_tracked_objects[i], track_token_ids, object_types)\n",
    "        output.append(tensorized)\n",
    "        output_types.append(agent_types)\n",
    "\n",
    "    return output, output_types\n",
    "\n",
    "def global_velocity_to_local(velocity, anchor_heading):\n",
    "    velocity_x = velocity[:, 0] * torch.cos(anchor_heading) + velocity[:, 1] * torch.sin(anchor_heading)\n",
    "    velocity_y = velocity[:, 1] * torch.cos(anchor_heading) - velocity[:, 0] * torch.sin(anchor_heading)\n",
    "\n",
    "    return torch.stack([velocity_x, velocity_y], dim=-1)\n",
    "\n",
    "def convert_absolute_quantities_to_relative(agent_state, ego_state, agent_type='ego'):\n",
    "    \"\"\"\n",
    "    Converts the agent' poses and relative velocities from absolute to ego-relative coordinates.\n",
    "    :param agent_state: The agent states to convert, in the AgentInternalIndex schema.\n",
    "    :param ego_state: The ego state to convert, in the EgoInternalIndex schema.\n",
    "    :return: The converted states, in AgentInternalIndex schema.\n",
    "    \"\"\"\n",
    "    ego_pose = torch.tensor(\n",
    "        [\n",
    "            float(ego_state[EgoInternalIndex.x()].item()),\n",
    "            float(ego_state[EgoInternalIndex.y()].item()),\n",
    "            float(ego_state[EgoInternalIndex.heading()].item()),\n",
    "        ],\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "\n",
    "    if agent_type == 'ego':\n",
    "        agent_global_poses = agent_state[:, [EgoInternalIndex.x(), EgoInternalIndex.y(), EgoInternalIndex.heading()]]\n",
    "        transformed_poses = global_state_se2_tensor_to_local(agent_global_poses, ego_pose, precision=torch.float64)\n",
    "        agent_state[:, EgoInternalIndex.x()] = transformed_poses[:, 0].float()\n",
    "        agent_state[:, EgoInternalIndex.y()] = transformed_poses[:, 1].float()\n",
    "        agent_state[:, EgoInternalIndex.heading()] = transformed_poses[:, 2].float()\n",
    "    else:\n",
    "        agent_global_poses = agent_state[:, [AgentInternalIndex.x(), AgentInternalIndex.y(), AgentInternalIndex.heading()]]\n",
    "        agent_global_velocities = agent_state[:, [AgentInternalIndex.vx(), AgentInternalIndex.vy()]]\n",
    "        transformed_poses = global_state_se2_tensor_to_local(agent_global_poses, ego_pose, precision=torch.float64)\n",
    "        transformed_velocities = global_velocity_to_local(agent_global_velocities, ego_pose[-1])\n",
    "        agent_state[:, AgentInternalIndex.x()] = transformed_poses[:, 0].float()\n",
    "        agent_state[:, AgentInternalIndex.y()] = transformed_poses[:, 1].float()\n",
    "        agent_state[:, AgentInternalIndex.heading()] = transformed_poses[:, 2].float()\n",
    "        agent_state[:, AgentInternalIndex.vx()] = transformed_velocities[:, 0].float()\n",
    "        agent_state[:, AgentInternalIndex.vy()] = transformed_velocities[:, 1].float()\n",
    "\n",
    "    return agent_state\n",
    "\n",
    "def agent_past_process(anchor_ego_state, past_time_stamps, past_tracked_objects, tracked_objects_types, num_agents):\n",
    "    \"\"\"\n",
    "    This function process the data from the raw agent data.\n",
    "    :param past_time_stamps: The input tensor data of the past timestamps.\n",
    "    :param past_time_stamps: The input tensor data of other agents in the past.\n",
    "    :return: ego_agent_array, other_agents_array.\n",
    "    \"\"\"\n",
    "    agents_states_dim = Agents.agents_states_dim()\n",
    "    time_stamps = past_time_stamps\n",
    "    agents = past_tracked_objects\n",
    "\n",
    "    agent_history = filter_agents_tensor(agents, reverse=True)\n",
    "    agent_types = tracked_objects_types[-1]\n",
    "\n",
    "    \"\"\"\n",
    "    Model input feature representing the present and past states of the ego and agents, including:\n",
    "    ego: <np.ndarray: num_frames, 7>\n",
    "        The num_frames includes both present and past frames.\n",
    "        The last dimension is the ego pose (x, y, heading) velocities (vx, vy) acceleration (ax, ay) at time t.\n",
    "    agents: <np.ndarray: num_frames, num_agents, 8>\n",
    "        Agent features indexed by agent feature type.\n",
    "        The num_frames includes both present and past frames.\n",
    "        The num_agents is padded to fit the largest number of agents across all frames.\n",
    "        The last dimension is the agent pose (x, y, heading) velocities (vx, vy, yaw rate) and size (length, width) at time t.\n",
    "    \"\"\"\n",
    "\n",
    "    if agent_history[-1].shape[0] == 0:\n",
    "        # Return zero tensor when there are no agents in the scene\n",
    "        agents_tensor = torch.zeros((len(agent_history), 0, agents_states_dim)).float()\n",
    "    else:\n",
    "        local_coords_agent_states = []\n",
    "        padded_agent_states = pad_agent_states(agent_history, reverse=True)\n",
    "\n",
    "        for agent_state in padded_agent_states:\n",
    "            local_coords_agent_states.append(convert_absolute_quantities_to_relative(agent_state, anchor_ego_state, 'agent'))\n",
    "    \n",
    "        # Calculate yaw rate\n",
    "        yaw_rate_horizon = compute_yaw_rate_from_state_tensors(padded_agent_states, time_stamps)\n",
    "    \n",
    "        agents_tensor = pack_agents_tensor(local_coords_agent_states, yaw_rate_horizon)\n",
    "\n",
    "    '''\n",
    "    Post-process the agents tensor to select a fixed number of agents closest to the ego vehicle.\n",
    "    agents: <np.ndarray: num_agents, num_frames, 11>]].\n",
    "        Agent type is one-hot encoded: [1, 0, 0] vehicle, [0, 1, 0] pedestrian, [0, 0, 1] bicycle \n",
    "            and added to the feature of the agent\n",
    "        The num_agents is padded or trimmed to fit the predefined number of agents across.\n",
    "        The num_frames includes both present and past frames.\n",
    "    '''\n",
    "    agents = np.zeros(shape=(num_agents, agents_tensor.shape[0], agents_tensor.shape[-1]+3), dtype=np.float32)\n",
    "\n",
    "    # sort agents according to distance to ego\n",
    "    distance_to_ego = torch.norm(agents_tensor[-1, :, :2], dim=-1)\n",
    "    indices = list(torch.argsort(distance_to_ego).numpy())[:num_agents]\n",
    "\n",
    "    # fill agent features into the array\n",
    "    for i, j in enumerate(indices):\n",
    "        agents[i, :, :agents_tensor.shape[-1]] = agents_tensor[:, j, :agents_tensor.shape[-1]].numpy()\n",
    "        if agent_types[j] == TrackedObjectType.VEHICLE:\n",
    "            agents[i, :, agents_tensor.shape[-1]:] = [1, 0, 0]\n",
    "        elif agent_types[j] == TrackedObjectType.PEDESTRIAN:\n",
    "            agents[i, :, agents_tensor.shape[-1]:] = [0, 1, 0]\n",
    "        else:\n",
    "            agents[i, :, agents_tensor.shape[-1]:] = [0, 0, 1]\n",
    "\n",
    "    return agents, indices\n",
    "\n",
    "def agent_future_process(anchor_ego_state, future_tracked_objects, num_agents, agent_index):\n",
    "    anchor_ego_state = torch.tensor([anchor_ego_state.rear_axle.x, anchor_ego_state.rear_axle.y, anchor_ego_state.rear_axle.heading, \n",
    "                                     anchor_ego_state.dynamic_car_state.rear_axle_velocity_2d.x,\n",
    "                                     anchor_ego_state.dynamic_car_state.rear_axle_velocity_2d.y,\n",
    "                                     anchor_ego_state.dynamic_car_state.rear_axle_acceleration_2d.x,\n",
    "                                     anchor_ego_state.dynamic_car_state.rear_axle_acceleration_2d.y])\n",
    "    \n",
    "    print(\"filter before\", len(future_tracked_objects))\n",
    "    \n",
    "    agent_future = filter_agents_tensor(future_tracked_objects)\n",
    "    \n",
    "    print(\"filter after\", len(agent_future))\n",
    "\n",
    "\n",
    "    local_coords_agent_states = []\n",
    "    \n",
    "    for agent_state in agent_future:\n",
    "        local_coords_agent_states.append(\n",
    "            convert_absolute_quantities_to_relative(\n",
    "                agent_state, \n",
    "                anchor_ego_state, \n",
    "                'agent'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    padded_agent_states = pad_agent_states_with_zeros(local_coords_agent_states)\n",
    "\n",
    "    # fill agent features into the array\n",
    "    agent_futures = np.zeros(\n",
    "        shape=(num_agents, padded_agent_states.shape[0]-1, 3), \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    for i, j in enumerate(agent_index):\n",
    "        agent_futures[i] = padded_agent_states[\n",
    "            1:, \n",
    "            j, \n",
    "            [AgentInternalIndex.x(), AgentInternalIndex.y(), AgentInternalIndex.heading()]\n",
    "        ].numpy()\n",
    "\n",
    "    return agent_futures\n",
    "\n",
    "def pad_agent_states_with_zeros(agent_trajectories):\n",
    "    key_frame = agent_trajectories[0]\n",
    "    track_id_idx = AgentInternalIndex.track_token()\n",
    "\n",
    "    pad_agent_trajectories = torch.zeros((len(agent_trajectories), key_frame.shape[0], key_frame.shape[1]), dtype=torch.float32)\n",
    "    for idx in range(len(agent_trajectories)):\n",
    "        frame = agent_trajectories[idx]\n",
    "        mapped_rows = frame[:, track_id_idx]\n",
    "\n",
    "        for row_idx in range(key_frame.shape[0]):\n",
    "            if row_idx in mapped_rows:\n",
    "                pad_agent_trajectories[idx, row_idx] = frame[frame[:, track_id_idx]==row_idx]\n",
    "\n",
    "    return pad_agent_trajectories\n",
    "\n",
    "def get_neighbor_vector_set_map(\n",
    "    map_api: AbstractMap,\n",
    "    map_features: List[str],\n",
    "    point: Point2D,\n",
    "    radius: float,\n",
    "    route_roadblock_ids: List[str],\n",
    "    traffic_light_status_data: List[TrafficLightStatusData],\n",
    ") -> Tuple[Dict[str, MapObjectPolylines], Dict[str, LaneSegmentTrafficLightData]]:\n",
    "    \"\"\"\n",
    "    Extract neighbor vector set map information around ego vehicle.\n",
    "    :param map_api: map to perform extraction on.\n",
    "    :param map_features: Name of map features to extract.\n",
    "    :param point: [m] x, y coordinates in global frame.\n",
    "    :param radius: [m] floating number about vector map query range.\n",
    "    :param route_roadblock_ids: List of ids of roadblocks/roadblock connectors (lane groups) within goal route.\n",
    "    :param traffic_light_status_data: A list of all available data at the current time step.\n",
    "    :return:\n",
    "        coords: Dictionary mapping feature name to polyline vector sets.\n",
    "        traffic_light_data: Dictionary mapping feature name to traffic light info corresponding to map elements\n",
    "            in coords.\n",
    "    :raise ValueError: if provided feature_name is not a valid VectorFeatureLayer.\n",
    "    \"\"\"\n",
    "    coords: Dict[str, MapObjectPolylines] = {}\n",
    "    traffic_light_data: Dict[str, LaneSegmentTrafficLightData] = {}\n",
    "    feature_layers: List[VectorFeatureLayer] = []\n",
    "\n",
    "    for feature_name in map_features:\n",
    "        try:\n",
    "            feature_layers.append(VectorFeatureLayer[feature_name])\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Object representation for layer: {feature_name} is unavailable\")\n",
    "\n",
    "    # extract lanes\n",
    "    if VectorFeatureLayer.LANE in feature_layers:\n",
    "        lanes_mid, lanes_left, lanes_right, lane_ids = get_lane_polylines(map_api, point, radius)\n",
    "\n",
    "        # lane baseline paths\n",
    "        coords[VectorFeatureLayer.LANE.name] = lanes_mid\n",
    "\n",
    "        # lane traffic light data\n",
    "        traffic_light_data[VectorFeatureLayer.LANE.name] = get_traffic_light_encoding(\n",
    "            lane_ids, traffic_light_status_data\n",
    "        )\n",
    "\n",
    "        # lane boundaries\n",
    "        if VectorFeatureLayer.LEFT_BOUNDARY in feature_layers:\n",
    "            coords[VectorFeatureLayer.LEFT_BOUNDARY.name] = MapObjectPolylines(lanes_left.polylines)\n",
    "        if VectorFeatureLayer.RIGHT_BOUNDARY in feature_layers:\n",
    "            coords[VectorFeatureLayer.RIGHT_BOUNDARY.name] = MapObjectPolylines(lanes_right.polylines)\n",
    "\n",
    "    # extract route\n",
    "    if VectorFeatureLayer.ROUTE_LANES in feature_layers:\n",
    "        route_polylines = get_route_lane_polylines_from_roadblock_ids(\n",
    "            map_api, \n",
    "            point, \n",
    "            radius, \n",
    "            route_roadblock_ids\n",
    "        )\n",
    "        \n",
    "        coords[VectorFeatureLayer.ROUTE_LANES.name] = route_polylines\n",
    "\n",
    "    # extract generic map objects\n",
    "    for feature_layer in feature_layers:\n",
    "        if feature_layer in VectorFeatureLayerMapping.available_polygon_layers():\n",
    "            polygons = get_map_object_polygons(\n",
    "                map_api, point, radius, VectorFeatureLayerMapping.semantic_map_layer(feature_layer)\n",
    "            )\n",
    "            coords[feature_layer.name] = polygons\n",
    "\n",
    "    return coords, traffic_light_data\n",
    "\n",
    "def convert_feature_layer_to_fixed_size(ego_pose, feature_coords, feature_tl_data, max_elements, max_points,\n",
    "                                         traffic_light_encoding_dim, interpolation):\n",
    "    \"\"\"\n",
    "    Converts variable sized map features to fixed size tensors. Map elements are padded/trimmed to max_elements size.\n",
    "        Points per feature are interpolated to maintain max_points size.\n",
    "    :param ego_pose: the current pose of the ego vehicle.\n",
    "    :param feature_coords: Vector set of coordinates for collection of elements in map layer.\n",
    "        [num_elements, num_points_in_element (variable size), 2]\n",
    "    :param feature_tl_data: Optional traffic light status corresponding to map elements at given index in coords.\n",
    "        [num_elements, traffic_light_encoding_dim (4)]\n",
    "    :param max_elements: Number of elements to pad/trim to.\n",
    "    :param max_points: Number of points to interpolate or pad/trim to.\n",
    "    :param traffic_light_encoding_dim: Dimensionality of traffic light data.\n",
    "    :param interpolation: Optional interpolation mode for maintaining fixed number of points per element.\n",
    "        None indicates trimming and zero-padding to take place in lieu of interpolation. Interpolation options: 'linear' and 'area'.\n",
    "    :return\n",
    "        coords_tensor: The converted coords tensor.\n",
    "        tl_data_tensor: The converted traffic light data tensor (if available).\n",
    "        avails_tensor: Availabilities tensor identifying real vs zero-padded data in coords_tensor and tl_data_tensor.\n",
    "    :raise ValueError: If coordinates and traffic light data size do not match.\n",
    "    \"\"\"\n",
    "    if feature_tl_data is not None and len(feature_coords) != len(feature_tl_data):\n",
    "        raise ValueError(f\"Size between feature coords and traffic light data inconsistent: {len(feature_coords)}, {len(feature_tl_data)}\")\n",
    "\n",
    "    # trim or zero-pad elements to maintain fixed size\n",
    "    coords_tensor = torch.zeros((max_elements, max_points, 2), dtype=torch.float64)\n",
    "    avails_tensor = torch.zeros((max_elements, max_points), dtype=torch.bool)\n",
    "    tl_data_tensor = (\n",
    "        torch.zeros((max_elements, max_points, traffic_light_encoding_dim), dtype=torch.float64)\n",
    "        if feature_tl_data is not None else None\n",
    "    )\n",
    "\n",
    "    # get elements according to the mean distance to the ego pose\n",
    "    mapping = {}\n",
    "    for i, e in enumerate(feature_coords):\n",
    "        dist = torch.norm(e - ego_pose[None, :2], dim=-1).min()\n",
    "        mapping[i] = dist\n",
    "\n",
    "    mapping = sorted(mapping.items(), key=lambda item: item[1])\n",
    "    sorted_elements = mapping[:max_elements]\n",
    "\n",
    "    # pad or trim waypoints in a map element\n",
    "    for idx, element_idx in enumerate(sorted_elements):\n",
    "        element_coords = feature_coords[element_idx[0]]\n",
    "    \n",
    "        # interpolate to maintain fixed size if the number of points is not enough\n",
    "        element_coords = interpolate_points(element_coords, max_points, interpolation=interpolation)\n",
    "        coords_tensor[idx] = element_coords\n",
    "        avails_tensor[idx] = True  # specify real vs zero-padded data\n",
    "\n",
    "        if tl_data_tensor is not None and feature_tl_data is not None:\n",
    "            tl_data_tensor[idx] = feature_tl_data[element_idx[0]]\n",
    "\n",
    "    return coords_tensor, tl_data_tensor, avails_tensor\n",
    "\n",
    "def map_process(anchor_state, coords, traffic_light_data, map_features, max_elements, max_points, interpolation_method):\n",
    "    \"\"\"\n",
    "    This function process the data from the raw vector set map data.\n",
    "    :param anchor_state: The current state of the ego vehicle.\n",
    "    :param coords: The input data of the vectorized map coordinates.\n",
    "    :param traffic_light_data: The input data of the traffic light data.\n",
    "    :return: dict of the map elements.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert data to tensor list\n",
    "    anchor_state_tensor = torch.tensor([anchor_state.x, anchor_state.y, anchor_state.heading], dtype=torch.float64)\n",
    "    list_tensor_data = {}\n",
    "\n",
    "    for feature_name, feature_coords in coords.items():\n",
    "        list_feature_coords = []\n",
    "\n",
    "        # Pack coords into tensor list\n",
    "        for element_coords in feature_coords.to_vector():\n",
    "            list_feature_coords.append(torch.tensor(element_coords, dtype=torch.float64))\n",
    "        list_tensor_data[f\"coords.{feature_name}\"] = list_feature_coords\n",
    "\n",
    "        # Pack traffic light data into tensor list if it exists\n",
    "        if feature_name in traffic_light_data:\n",
    "            list_feature_tl_data = []\n",
    "\n",
    "            for element_tl_data in traffic_light_data[feature_name].to_vector():\n",
    "                list_feature_tl_data.append(torch.tensor(element_tl_data, dtype=torch.float64))\n",
    "            list_tensor_data[f\"traffic_light_data.{feature_name}\"] = list_feature_tl_data\n",
    "\n",
    "    \"\"\"\n",
    "    Vector set map data structure, including:\n",
    "    coords: Dict[str, List[<np.ndarray: num_elements, num_points, 2>]].\n",
    "            The (x, y) coordinates of each point in a map element across map elements per sample.\n",
    "    traffic_light_data: Dict[str, List[<np.ndarray: num_elements, num_points, 4>]].\n",
    "            One-hot encoding of traffic light status for each point in a map element across map elements per sample.\n",
    "            Encoding: green [1, 0, 0, 0] yellow [0, 1, 0, 0], red [0, 0, 1, 0], unknown [0, 0, 0, 1]\n",
    "    availabilities: Dict[str, List[<np.ndarray: num_elements, num_points>]].\n",
    "            Boolean indicator of whether feature data is available for point at given index or if it is zero-padded.\n",
    "    \"\"\"\n",
    "    \n",
    "    tensor_output = {}\n",
    "    traffic_light_encoding_dim = LaneSegmentTrafficLightData.encoding_dim()\n",
    "\n",
    "    for feature_name in map_features:\n",
    "        if f\"coords.{feature_name}\" in list_tensor_data:\n",
    "            feature_coords = list_tensor_data[f\"coords.{feature_name}\"]\n",
    "\n",
    "            feature_tl_data = (\n",
    "                list_tensor_data[f\"traffic_light_data.{feature_name}\"]\n",
    "                if f\"traffic_light_data.{feature_name}\" in list_tensor_data\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            coords, tl_data, avails = convert_feature_layer_to_fixed_size(\n",
    "                    anchor_state_tensor,\n",
    "                    feature_coords,\n",
    "                    feature_tl_data,\n",
    "                    max_elements[feature_name],\n",
    "                    max_points[feature_name],\n",
    "                    traffic_light_encoding_dim,\n",
    "                    interpolation=interpolation_method  # apply interpolation only for lane features\n",
    "                    if feature_name\n",
    "                    in [\n",
    "                        VectorFeatureLayer.LANE.name,\n",
    "                        VectorFeatureLayer.LEFT_BOUNDARY.name,\n",
    "                        VectorFeatureLayer.RIGHT_BOUNDARY.name,\n",
    "                        VectorFeatureLayer.ROUTE_LANES.name,\n",
    "                        VectorFeatureLayer.CROSSWALK.name\n",
    "                    ]\n",
    "                    else None,\n",
    "            )\n",
    "\n",
    "            coords = vector_set_coordinates_to_local_frame(coords, avails, anchor_state_tensor)\n",
    "\n",
    "            tensor_output[f\"vector_set_map.coords.{feature_name}\"] = coords\n",
    "            tensor_output[f\"vector_set_map.availabilities.{feature_name}\"] = avails\n",
    "\n",
    "            if tl_data is not None:\n",
    "                tensor_output[f\"vector_set_map.traffic_light_data.{feature_name}\"] = tl_data\n",
    "\n",
    "    \"\"\"\n",
    "    Post-precoss the map elements to different map types. Each map type is a array with the following shape.\n",
    "    N: number of map elements (fixed for a given map feature)\n",
    "    P: number of points (fixed for a given map feature)\n",
    "    F: number of features\n",
    "    \"\"\"\n",
    "\n",
    "    for feature_name in map_features:\n",
    "        if feature_name == \"LANE\":\n",
    "            polylines = tensor_output[f'vector_set_map.coords.{feature_name}'].numpy()\n",
    "            traffic_light_state = tensor_output[f'vector_set_map.traffic_light_data.{feature_name}'].numpy()\n",
    "            avails = tensor_output[f'vector_set_map.availabilities.{feature_name}'].numpy()\n",
    "            vector_map_lanes = polyline_process(polylines, avails, traffic_light_state)\n",
    "\n",
    "        elif feature_name == \"CROSSWALK\":\n",
    "            polylines = tensor_output[f'vector_set_map.coords.{feature_name}'].numpy()\n",
    "            avails = tensor_output[f'vector_set_map.availabilities.{feature_name}'].numpy()\n",
    "            vector_map_crosswalks = polyline_process(polylines, avails)\n",
    "\n",
    "        elif feature_name == \"ROUTE_LANES\":\n",
    "            polylines = tensor_output[f'vector_set_map.coords.{feature_name}'].numpy()\n",
    "            avails = tensor_output[f'vector_set_map.availabilities.{feature_name}'].numpy()\n",
    "            vector_map_route_lanes = polyline_process(polylines, avails)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    vector_map_output = {'lanes': vector_map_lanes, 'crosswalks': vector_map_crosswalks, 'route_lanes': vector_map_route_lanes}\n",
    "\n",
    "    return vector_map_output\n",
    "\n",
    "def polyline_process(polylines, avails, traffic_light=None):\n",
    "    dim = 3 if traffic_light is None else 7\n",
    "    new_polylines = np.zeros(shape=(polylines.shape[0], polylines.shape[1], dim), dtype=np.float64)\n",
    "\n",
    "    for i in range(polylines.shape[0]):\n",
    "        if avails[i][0]: \n",
    "            polyline = polylines[i]\n",
    "            polyline_heading = wrap_to_pi(np.arctan2(polyline[1:, 1]-polyline[:-1, 1], polyline[1:, 0]-polyline[:-1, 0]))\n",
    "            polyline_heading = np.insert(polyline_heading, -1, polyline_heading[-1])[:, np.newaxis]\n",
    "            if traffic_light is None:\n",
    "                new_polylines[i] = np.concatenate([polyline, polyline_heading], axis=-1)\n",
    "            else:\n",
    "                new_polylines[i] = np.concatenate([polyline, polyline_heading, traffic_light[i]], axis=-1)  \n",
    "\n",
    "    return new_polylines\n",
    "\n",
    "def wrap_to_pi(theta):\n",
    "    return (theta+np.pi) % (2*np.pi) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuplan arguments\n",
    "data_path = '/root/nuplan/dataset/nuplan-v1.1/splits/mini'\n",
    "map_path = '/root/nuplan/dataset/maps'\n",
    "save_path = '/root/workspace/GameFormer-Planner/ProcessedData'\n",
    "scenarios_per_type = 1000\n",
    "total_scenarios = None\n",
    "shuffle_scenarios = False\n",
    "debug = False\n",
    "\n",
    "map_version = \"nuplan-maps-v1.0\"    \n",
    "sensor_root = None\n",
    "db_files = None\n",
    "\n",
    "# create folder for processed data\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# get scenarios\n",
    "scenario_mapping = ScenarioMapping(\n",
    "    scenario_map=get_scenario_map(), \n",
    "    subsample_ratio_override=0.5\n",
    ")\n",
    "\n",
    "builder = NuPlanScenarioBuilder(\n",
    "    data_path, \n",
    "    map_path, \n",
    "    sensor_root, \n",
    "    db_files, \n",
    "    map_version, \n",
    "    scenario_mapping=scenario_mapping\n",
    ")\n",
    "\n",
    "# scenarios for training\n",
    "scenario_filter = ScenarioFilter(\n",
    "    *get_filter_parameters(\n",
    "        scenarios_per_type, \n",
    "        total_scenarios, \n",
    "        shuffle_scenarios\n",
    "    )\n",
    ")\n",
    "\n",
    "# enable parallel process\n",
    "worker = SingleMachineParallelExecutor(use_process_pool=True)\n",
    "\n",
    "# get scenarios\n",
    "scenarios = builder.get_scenarios(scenario_filter, worker)\n",
    "\n",
    "# delete useless variables, only deal with scenarios\n",
    "del worker, builder, scenario_filter, scenario_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter before 81\n",
      "filter after 81\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nuplan.planning.scenario_builder.abstract_scenario import AbstractScenario\n",
    "from nuplan.common.actor_state.ego_state import EgoState\n",
    "from nuplan.common.actor_state.vehicle_parameters import get_pacifica_parameters\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "class DataProcessor:\n",
    "    scenario: AbstractScenario\n",
    "    anchor_ego_state: EgoState\n",
    " \n",
    "    def __init__(self, scenario):\n",
    "        # map api\n",
    "        self.scenario = scenario\n",
    "        \n",
    "        # discrete 1 second to several points\n",
    "        self.discrete_size = 10\n",
    "\n",
    "        # history observation horizon\n",
    "        self.past_time_horizon = 2\n",
    "\n",
    "        # prediction horizon\n",
    "        self.future_time_horizon = 8\n",
    "\n",
    "        # number of discrete trajectory points\n",
    "        self.num_past_poses = self.discrete_size * self.past_time_horizon\n",
    "\n",
    "        # number of discrete trajectory points in future horizon\n",
    "        self.num_future_poses = self.discrete_size * self.future_time_horizon\n",
    "\n",
    "        # deal with top 20 closest agents around the ego vehicle\n",
    "        self.num_agents = 20\n",
    "\n",
    "        # [m] query radius scope relative to the current pose.\n",
    "        self.query_radius = 60\n",
    "\n",
    "        # [m/s] velocity threshold for determine weather an agent is static.\n",
    "        # FIXME: Unused\n",
    "        self.velocity_thresh = 2\n",
    "  \n",
    "        # Interpolation method to apply when interpolating to maintain fixed size map elements.\n",
    "        self.interpolation_method = 'linear'\n",
    "\n",
    "        # name of map features to be extracted.\n",
    "        self.map_features = [\n",
    "            'LANE', \n",
    "            'ROUTE_LANES', \n",
    "            'CROSSWALK'\n",
    "        ] \n",
    "\n",
    "        # maximum number of elements to extract per feature layer.\n",
    "        self.max_elements = {\n",
    "            'LANE': 40, \n",
    "            'ROUTE_LANES': 10, \n",
    "            'CROSSWALK': 5\n",
    "        }      \n",
    "        \n",
    "        # maximum number of points per feature to extract per feature layer.\n",
    "        self.max_points = {\n",
    "            'LANE': 50, \n",
    "            'ROUTE_LANES': 50, \n",
    "            'CROSSWALK': 30\n",
    "        }\n",
    "\n",
    "        # initial state of ego vehicle\n",
    "        self.anchor_ego_state = self.scenario.initial_ego_state\n",
    "  \n",
    "        # map related\n",
    "        self.map_name = self.scenario._map_name\n",
    "        self.map_api = scenario.map_api\n",
    "\n",
    "        # debug\n",
    "        self.debug = True\n",
    "  \n",
    "    def process_ego_info(self):\n",
    "        # get history states\n",
    "        past_ego_states = self.scenario.get_ego_past_trajectory(\n",
    "            iteration=0,\n",
    "            num_samples=self.num_past_poses,\n",
    "            time_horizon=self.past_time_horizon\n",
    "        )\n",
    "  \n",
    "        sampled_past_ego_states = list(past_ego_states) + [self.anchor_ego_state]\n",
    "\n",
    "        past_ego_relative_states = convert_absolute_to_relative_poses(\n",
    "            self.anchor_ego_state.rear_axle,\n",
    "            [state.rear_axle for state in sampled_past_ego_states]\n",
    "        )\n",
    "\n",
    "        # get history timestamps\n",
    "        past_timestamps = self.scenario.get_past_timestamps(\n",
    "                iteration=0, \n",
    "                num_samples=self.num_past_poses, \n",
    "                   time_horizon=self.past_time_horizon\n",
    "        )\t\n",
    "\n",
    "        sampled_past_timestamps = list(past_timestamps) + [self.scenario.start_time]\n",
    "        initial_timestamp = sampled_past_timestamps[0].time_s\n",
    "\n",
    "        past_timestamp_list = [timestamp.time_s - initial_timestamp for timestamp in sampled_past_timestamps]\n",
    "\n",
    "        # get future states\n",
    "        future_ego_states = self.scenario.get_ego_future_trajectory(\n",
    "            iteration=0, \n",
    "             num_samples=self.num_future_poses, \n",
    "              time_horizon=self.future_time_horizon\n",
    "        )\n",
    "\n",
    "        future_ego_relative_states = convert_absolute_to_relative_poses(\n",
    "            self.anchor_ego_state.rear_axle,\n",
    "            [state.rear_axle for state in future_ego_states]\n",
    "        )\n",
    "\n",
    "        # get future timestamps\n",
    "        future_timestamps = self.scenario.get_future_timestamps(\n",
    "            iteration=0,\n",
    "            num_samples=self.num_future_poses,\n",
    "            time_horizon=self.future_time_horizon\n",
    "        )\n",
    "\n",
    "        future_timestamp_list = [timestamp.time_s - initial_timestamp for timestamp in future_timestamps]\n",
    "\n",
    "        # debug\n",
    "        if self.debug:\n",
    "            plot_ego(past_ego_relative_states, future_ego_relative_states)\n",
    "  \n",
    "        return past_ego_relative_states, future_ego_relative_states, past_timestamp_list, future_timestamp_list\n",
    "\n",
    "    def process_agent_info(self):\n",
    "        # get tracked objects at current time\n",
    "        present_tracked_objects = self.scenario.initial_tracked_objects.tracked_objects\n",
    "  \n",
    "        # get tracked obejcts at previous horizon\n",
    "        past_tracked_objects = [\n",
    "            tracked_objects.tracked_objects\n",
    "            for tracked_objects in self.scenario.get_past_tracked_objects(\n",
    "                iteration=0, \n",
    "                time_horizon=self.past_time_horizon, \n",
    "                   num_samples=self.num_past_poses\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # get observed objects\n",
    "        sampled_past_observations = past_tracked_objects + [present_tracked_objects]\n",
    "        \n",
    "        # pack to tensor\n",
    "        past_tracked_objects_tensor_list, past_tracked_objects_types = \\\n",
    "              sampled_tracked_objects_to_tensor_list(sampled_past_observations)\n",
    "\n",
    "        # get timestamp\n",
    "        past_time_stamps = list(\n",
    "            self.scenario.get_past_timestamps(\n",
    "                iteration=0, num_samples=self.num_past_poses, time_horizon=self.past_time_horizon\n",
    "            )\n",
    "        ) + [self.scenario.start_time]\n",
    "        \n",
    "        # turn timestamp list into tensor\n",
    "        past_time_stamps_tensor = sampled_past_timestamps_to_tensor(past_time_stamps)\n",
    "\n",
    "        # get anchor_ego_state in the form of list [x, y, θ]\n",
    "        anchor_ego_state_list = [0] * Agents.agents_states_dim()\n",
    "        anchor_ego_state_list[EgoInternalIndex.x()] = self.anchor_ego_state.rear_axle.x\n",
    "        anchor_ego_state_list[EgoInternalIndex.y()] = self.anchor_ego_state.rear_axle.y\n",
    "        anchor_ego_state_list[EgoInternalIndex.heading()] = self.anchor_ego_state.rear_axle.heading\n",
    "\n",
    "        # 1. Filter detections to keep only agents which appear in the last frame\n",
    "        # 2. Turn the observation into tensor: \n",
    "        #\t\t[x, y, θ, vx, vy, θ', length, width, agent type]\n",
    "        #    Agent type is one-hot encoded: \n",
    "        # \t\t[1, 0, 0] vehicle, \n",
    "        #\t\t[0, 1, 0] pedestrian, \n",
    "        #\t \t[0, 0, 1] bicycle\n",
    "        neighbor_agents_past, agent_indices = agent_past_process(\n",
    "            anchor_ego_state_list, \n",
    "            past_time_stamps_tensor, \n",
    "            past_tracked_objects_tensor_list, \n",
    "            past_tracked_objects_types, \n",
    "            self.num_agents\n",
    "        )\n",
    "\n",
    "        # get future tracked objects\n",
    "        future_tracked_objects = [\n",
    "            tracked_objects.tracked_objects\n",
    "            for tracked_objects in self.scenario.get_future_tracked_objects(\n",
    "                iteration=0, \n",
    "                time_horizon=self.future_time_horizon, \n",
    "                num_samples=self.num_future_poses\n",
    "            )\t\t\t\n",
    "        ]\n",
    "\n",
    "        # get prediction\n",
    "        sampled_future_observations = [present_tracked_objects] + future_tracked_objects\n",
    "\n",
    "        # turn list into tensor\n",
    "        future_tracked_objects_tensor_list, _ = \\\n",
    "            sampled_tracked_objects_to_tensor_list(sampled_future_observations)\n",
    "\n",
    "        # 1. Filter detections to keep only agents which appear in the first frame\n",
    "        # 2. Turn the observation into tensor [x, y, θ]\n",
    "        neighbor_agent_futures = agent_future_process(\n",
    "              self.anchor_ego_state, \n",
    "            future_tracked_objects_tensor_list, \n",
    "             self.num_agents, \n",
    "              agent_indices\n",
    "        )\n",
    "\n",
    "        if self.debug:\n",
    "            plot_agent(neighbor_agents_past, neighbor_agent_futures, agent_indices)\n",
    "  \n",
    "        return neighbor_agents_past, neighbor_agent_futures\n",
    "\n",
    "    def process_map_info(self):\n",
    "        # convert state to 2D pose\n",
    "        ego_coords = Point2D(\n",
    "              self.anchor_ego_state.rear_axle.x, \n",
    "            self.anchor_ego_state.rear_axle.y\n",
    "        )\n",
    "\n",
    "        # get roadblock id list\n",
    "        route_roadblock_ids = self.scenario.get_route_roadblock_ids()\n",
    "        \n",
    "        # get traffic light data\n",
    "        traffic_light_data = self.scenario.get_traffic_light_status_at_iteration(0)\n",
    "        \n",
    "        # get neighbor vector set and \n",
    "        coords, traffic_light_data = get_neighbor_vector_set_map(\n",
    "            self.map_api,\n",
    "            self.map_features, \n",
    "            ego_coords,\n",
    "            self.query_radius,\n",
    "            route_roadblock_ids,\n",
    "            traffic_light_data\n",
    "        )\n",
    "\n",
    "        # get tensorized vector map dictionary\n",
    "        vector_map = map_process(\n",
    "              self.anchor_ego_state.rear_axle, \n",
    "            coords, \n",
    "             traffic_light_data, \n",
    "              self.map_features, \n",
    "            self.max_elements, \n",
    "            self.max_points, \n",
    "            self.interpolation_method\n",
    "        )\n",
    "\n",
    "        if self.debug:\n",
    "            plot_vector_map(\n",
    "                   vector_map['lanes'], \n",
    "                  vector_map['crosswalks'], \n",
    "                vector_map['route_lanes']\n",
    "            )\n",
    "        \n",
    "        # Dict('Type', List[x, y, heading, traffic_light_status(x, x, x, x)])\n",
    "        return vector_map\n",
    "\n",
    "COLOR_DICT = {\n",
    "    'centerline' : '#6495ED',\n",
    "    'crosswalk'  : '#778899',\n",
    "    'route_lane' : '#191970',\n",
    "    'agent'      : '#007672',\n",
    "    'ego'        : '#d33e4c',\n",
    "    'others'     : '#d3e8ef',\n",
    "}\n",
    "\n",
    "VELOCITY_THRESH = 1\n",
    "\n",
    "def plot_rectangle(x_center, y_center, heading, length, width, color):\n",
    "    agent_bottom_right = (\n",
    "        x_center - length/2, \n",
    "        y_center - width/2\n",
    "    )\n",
    "    \n",
    "    rect = plt.Rectangle(\n",
    "        agent_bottom_right, \n",
    "        length, \n",
    "        width, \n",
    "        linewidth=1, \n",
    "        color=color, \n",
    "        alpha=0.6, \n",
    "        zorder=3,\n",
    "        transform=mpl.transforms.Affine2D().rotate_around(*(x_center, y_center), heading) + plt.gca().transData\n",
    "    )\n",
    "    plt.gca().add_patch(rect)    \n",
    "\n",
    "def plot_vector_map(lanes, crosswalks, route_lanes):\n",
    "    # plot centerline\n",
    "    for i in range(lanes.shape[0]):\n",
    "        lane = lanes[i]\n",
    "        if lane[0][0] != 0:\n",
    "            plt.plot(\n",
    "                   lane[:, 0], \n",
    "                  lane[:, 1], \n",
    "                color=COLOR_DICT['centerline'], \n",
    "                 linewidth=1\n",
    "            ) \n",
    "\n",
    "     # plot crosswalk\n",
    "    for j in range(crosswalks.shape[0]):\n",
    "        crosswalk = crosswalks[j]\n",
    "        if crosswalk[0][0] != 0:\n",
    "            plt.plot(\n",
    "                   crosswalk[:, 0], \n",
    "                  crosswalk[:, 1], \n",
    "                color=COLOR_DICT['crosswalk'], \n",
    "                 linewidth=1\n",
    "            ) \n",
    "\n",
    "     # plot route_lanes\n",
    "    for k in range(route_lanes.shape[0]):\n",
    "        route_lane = route_lanes[k]\n",
    "        if route_lane[0][0] != 0:\n",
    "            plt.plot(\n",
    "                   route_lane[:, 0], \n",
    "                  route_lane[:, 1], \n",
    "                color=COLOR_DICT['route_lane'], \n",
    "                 linewidth=1\n",
    "            ) \n",
    "\n",
    "    plt.axis('equal')\n",
    "\n",
    "def plot_ego(past_coords, future_coords):\n",
    "    # plot history trajectory\n",
    "    plt.plot(\n",
    "        past_coords[:, 0], \n",
    "          past_coords[:, 1], \n",
    "        color=COLOR_DICT['ego'], \n",
    "         linewidth=3)\n",
    " \n",
    "    # plot future trajectory\n",
    "    plt.plot(\n",
    "        future_coords[:, 0], \n",
    "         future_coords[:, 1], \n",
    "          color=COLOR_DICT['ego'], \n",
    "           linewidth=3\n",
    "    )\n",
    "    \n",
    "    # plot query point\n",
    "    vehicle_parameters = get_pacifica_parameters()\n",
    "    plot_rectangle(\n",
    "        x_center=past_coords[-1, 0],\n",
    "        y_center=past_coords[-1, 1],\n",
    "        heading=past_coords[-1, 2],\n",
    "        length=vehicle_parameters.front_length + vehicle_parameters.rear_length,\n",
    "        width=vehicle_parameters.width,\n",
    "        color=COLOR_DICT['ego']\n",
    "    )\n",
    "\n",
    "    plt.axis('equal')\n",
    "\n",
    "def plot_agent(agents_history, agents_future, agent_index):\n",
    "    for i in range(len(agent_index)):\n",
    "        agent = agents_history[i]\n",
    "  \n",
    "        if agent[0, 0] == 0:\n",
    "            continue\n",
    "        \n",
    "        vx = agent[:, 3]\n",
    "        vy = agent[:, 4]\n",
    "        speeds = np.sqrt(vx**2 + vy**2)\n",
    "        mean_speed = np.mean(speeds)\n",
    "        \n",
    "        agent_type = agent[0, 8:11]\n",
    "    \n",
    "        if (agent_type == np.array([1, 0, 0])).all() or mean_speed < VELOCITY_THRESH:\n",
    "            agent_color = COLOR_DICT['agent']\n",
    "        else:\n",
    "            agent_color = COLOR_DICT['others']\n",
    "\n",
    "        plt.plot(\n",
    "            agents_history[i, :, 0], \n",
    "            agents_history[i, :, 1], \n",
    "            color=agent_color, \n",
    "            linewidth=3\n",
    "        )\n",
    "\n",
    "        plt.plot(\n",
    "            agents_future[i, :, 0], \n",
    "            agents_future[i, :, 1], \n",
    "            color=agent_color, \n",
    "            linewidth=3\n",
    "        )\n",
    "            \n",
    "        plot_rectangle(\n",
    "            x_center=agent[-1, 0],\n",
    "            y_center=agent[-1, 1],\n",
    "            heading=agent[-1, 2],\n",
    "            length=agent[-1, 6],\n",
    "            width=agent[-1, 7],\n",
    "            color=agent_color\n",
    "        )\n",
    "  \n",
    "    plt.axis('equal')\n",
    "\n",
    "# pick a scenario\n",
    "scenario = scenarios[0]\n",
    "\n",
    "# process data\n",
    "data_processor = DataProcessor(scenario)\n",
    "vector_map = data_processor.process_map_info()\n",
    "ego_past, ego_future, pt, ft = data_processor.process_ego_info()\n",
    "agents_history, agents_future = data_processor.process_agent_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def agent_ground_truth_traj_to_increment(origin, agent_future):\n",
    "    origin_x = origin[0]\n",
    "    origin_y = origin[1]\n",
    "\n",
    "    start_x = agent_future[ :-1, 0].reshape(-1, 1)\n",
    "    start_y = agent_future[ :-1, 1].reshape(-1, 1)\n",
    "    end_x   = agent_future[1:  , 0].reshape(-1, 1)\n",
    "    end_y   = agent_future[1:  , 1].reshape(-1, 1)\n",
    "\n",
    "    increment_start_x = start_x[0, 0] - origin_x\n",
    "    increment_start_y = start_y[0, 0] - origin_y\n",
    "\n",
    "    increment_x = end_x - start_x\n",
    "    increment_y = end_y - start_y\n",
    "\n",
    "    ground_truth = np.hstack((\n",
    "        increment_x,\n",
    "        increment_y\n",
    "    ))\n",
    "\n",
    "    ground_truth = np.vstack((\n",
    "        np.array([increment_start_x, increment_start_y]),\n",
    "        ground_truth\n",
    "    ))\n",
    " \n",
    "    return ground_truth\n",
    "\n",
    "def encoding_feature(\n",
    "    agent_past, \n",
    "    agent_future, \n",
    "    ego_past,\n",
    "    ego_future,\n",
    "    vector_map\n",
    "    ):\n",
    "    # each subgraph has its only ID\n",
    "    subgraph_ID = 0\n",
    "    \n",
    "    # each subgraph ID has a series of node. These nodes are stacked \n",
    "    # vertically in a list and there has a start node and end node.  \n",
    "    trajectory_ID_to_indices = {}\n",
    "    map_ID_to_indices = {}\n",
    "\n",
    "    # ground truth for vectornet\n",
    "    ground_truth = agent_future\n",
    "    \n",
    "    # create an empty trajectory subgraph\n",
    "    # each node is a 1 * 8 tensor \n",
    "    # [start_x, start_y, end_x, end_y, x, x, x, subgraph_ID]\n",
    "    # agent type is one hot encoded\n",
    "    trajectory_subgraph = np.empty((0, 8))\n",
    "    \n",
    "    # create an empty map subgraph\n",
    "    # each node is a 1 * 8 tensor\n",
    "    # [start_x, start_y, end_x, end_y, x, x, x, subgraph_ID]\n",
    "    # lane type is one hot encoded\n",
    "    map_subgraph = np.empty((0, 8))\n",
    "    \n",
    "    # encoding ego features\n",
    "    subgraph_ID = 0\n",
    "    ego_subgraph = np.hstack((\n",
    "        ego_past[ :-1, 0].reshape(-1, 1),   \n",
    "        ego_past[ :-1, 1].reshape(-1, 1),      \n",
    "        ego_past[1:  , 0].reshape(-1, 1),\n",
    "        ego_past[1:  , 1].reshape(-1, 1),  \n",
    "        np.zeros((len(ego_past) - 1, 1)),\n",
    "        np.zeros((len(ego_past) - 1, 1)), \n",
    "        np.zeros((len(ego_past) - 1, 1)),  \n",
    "        np.ones ((len(ego_past) - 1, 1)) * subgraph_ID\n",
    "    ))\n",
    "    \n",
    "    trajectory_subgraph = np.vstack((\n",
    "        trajectory_subgraph,\n",
    "        ego_subgraph\n",
    "    ))\n",
    "    \n",
    "    start_ID = 0\n",
    "    end_ID = len(ego_past) - 2\n",
    "    trajectory_ID_to_indices[subgraph_ID] = (start_ID, end_ID)\n",
    "    subgraph_ID += 1\n",
    "    \n",
    "    # encoding ego ground truth\n",
    "    ground_truth = np.empty((0, 2))\n",
    "    ego_ground_truth = agent_ground_truth_traj_to_increment(\n",
    "         ego_past[-1], \n",
    "          ego_future\n",
    "    )\n",
    "    \n",
    "    ground_truth = np.vstack((\n",
    "        ground_truth,\n",
    "        ego_ground_truth\n",
    "    ))\n",
    " \n",
    "    # encoding agent features\n",
    "    for i in range(len(agent_past)):\n",
    "        agent = agent_past[i]\n",
    "  \n",
    "        agent_subgraph = np.hstack((\n",
    "            agent[ :-1, 0].reshape(-1, 1),    \n",
    "            agent[ :-1, 1].reshape(-1, 1),     \n",
    "            agent[1:  , 0].reshape(-1, 1),\n",
    "            agent[1:  , 1].reshape(-1, 1),  \n",
    "            np.zeros((len(agent) - 1, 1)),\n",
    "            np.zeros((len(agent) - 1, 1)), \n",
    "            np.zeros((len(agent) - 1, 1)),  \n",
    "            np.ones ((len(agent) - 1, 1)) * subgraph_ID            \n",
    "        ))\n",
    "        \n",
    "        trajectory_subgraph = np.vstack((\n",
    "            trajectory_subgraph,\n",
    "            agent_subgraph\n",
    "        ))\n",
    "        \n",
    "        start_ID = end_ID + 1\n",
    "        end_ID += len(agent_subgraph)\n",
    "\n",
    "        trajectory_ID_to_indices[subgraph_ID] = (start_ID, end_ID)\n",
    "        subgraph_ID += 1\n",
    "\n",
    "        # ground truth\n",
    "        origin = agent[-1]\n",
    "        future = agent_future[i]\n",
    "\n",
    "        agent_ground_truth = agent_ground_truth_traj_to_increment(\n",
    "            origin,\n",
    "            future\n",
    "        )\n",
    "\n",
    "        ground_truth = np.vstack((\n",
    "            ground_truth,\n",
    "            agent_ground_truth\n",
    "        ))\n",
    "  \n",
    "    # encoding lane features\n",
    "    start_ID = -1\n",
    "    end_ID = -1\n",
    "\n",
    "    # encoding centerline features\n",
    "    for lane in vector_map['lanes']:\n",
    "        lane_subgraph = np.hstack((\n",
    "            lane[ :-1, 0].reshape(-1, 1),  \n",
    "            lane[ :-1, 1].reshape(-1, 1),       \n",
    "            lane[1:  , 0].reshape(-1, 1),\n",
    "            lane[1:  , 1].reshape(-1, 1),  \n",
    "            np.zeros((len(lane) - 1, 1)),\n",
    "            np.zeros((len(lane) - 1, 1)), \n",
    "            np.zeros((len(lane) - 1, 1)),  \n",
    "            np.ones ((len(lane) - 1, 1)) * subgraph_ID                  \n",
    "        ))\n",
    "\n",
    "        map_subgraph = np.vstack((\n",
    "            map_subgraph,\n",
    "            lane_subgraph\n",
    "        ))\n",
    "  \n",
    "        start_ID = end_ID + 1\n",
    "        end_ID += len(lane_subgraph)\n",
    "  \n",
    "        map_ID_to_indices[subgraph_ID] = (start_ID, end_ID)\n",
    "        subgraph_ID += 1\n",
    "  \n",
    "    # encoding crosswalk features\n",
    "    for lane in vector_map['crosswalks']:\n",
    "        lane_subgraph = np.hstack((\n",
    "            lane[ :-1, 0].reshape(-1, 1),  \n",
    "            lane[ :-1, 1].reshape(-1, 1),       \n",
    "            lane[1:  , 0].reshape(-1, 1),\n",
    "            lane[1:  , 1].reshape(-1, 1),  \n",
    "            np.zeros((len(lane) - 1, 1)),\n",
    "            np.zeros((len(lane) - 1, 1)), \n",
    "            np.zeros((len(lane) - 1, 1)),  \n",
    "            np.ones ((len(lane) - 1, 1)) * subgraph_ID                  \n",
    "        ))\n",
    "\n",
    "        map_subgraph = np.vstack((\n",
    "            map_subgraph,\n",
    "            lane_subgraph\n",
    "        ))\n",
    "  \n",
    "        start_ID = end_ID + 1\n",
    "        end_ID += len(lane_subgraph)\n",
    "  \n",
    "        map_ID_to_indices[subgraph_ID] = (start_ID, end_ID)\n",
    "        subgraph_ID += 1\n",
    "\n",
    "    # encoding route lane features\n",
    "    for lane in vector_map['route_lanes']:\n",
    "        lane_subgraph = np.hstack((\n",
    "            lane[ :-1, 0].reshape(-1, 1),  \n",
    "            lane[ :-1, 1].reshape(-1, 1),       \n",
    "            lane[1:  , 0].reshape(-1, 1),\n",
    "            lane[1:  , 1].reshape(-1, 1),  \n",
    "            np.zeros((len(lane) - 1, 1)),\n",
    "            np.zeros((len(lane) - 1, 1)), \n",
    "            np.zeros((len(lane) - 1, 1)),  \n",
    "            np.ones ((len(lane) - 1, 1)) * subgraph_ID                  \n",
    "        ))\n",
    "  \n",
    "        map_subgraph = np.vstack((\n",
    "            map_subgraph,\n",
    "            lane_subgraph\n",
    "        ))\n",
    "  \n",
    "        start_ID = end_ID + 1\n",
    "        end_ID += len(lane_subgraph)\n",
    "  \n",
    "        map_ID_to_indices[subgraph_ID] = (start_ID, end_ID)\n",
    "        subgraph_ID += 1\n",
    "\n",
    "    # data frame\n",
    "    feature_data = [[\n",
    "        np.vstack((trajectory_subgraph, map_subgraph)),\n",
    "        ground_truth,\n",
    "        trajectory_ID_to_indices,\n",
    "        map_ID_to_indices,\n",
    "        trajectory_subgraph.shape[0],\n",
    "        map_subgraph.shape[0]\n",
    "    ]]\n",
    "    \n",
    "    return pd.DataFrame(\n",
    "        feature_data,\n",
    "        columns=[\n",
    "            \"POLYLINE_FEATURES\", \n",
    "            \"GROUND_TRUTH\",\n",
    "            \"TRAJ_ID_TO_INDICES\", \n",
    "            \"LANE_ID_TO_INDICES\", \n",
    "            \"TARJ_SIZE\", \n",
    "            \"LANE_SIZE\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "features_df = encoding_feature(\n",
    "    agents_history, \n",
    "    agents_future, \n",
    "    ego_past, \n",
    "    ego_future, \n",
    "    vector_map\n",
    ")\n",
    "\n",
    "def save_features(features_df, file_name, dir):\n",
    "    feature_file_name = f\"features_{file_name}.pkl\"\n",
    "    features_df.to_pickle(\n",
    "        os.path.join(dir, feature_file_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 80, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for agent in agents_future:\n",
    "#     plt.plot(agent[50:, 0], agent[50:, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuplan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
